{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_column(df, text_column, vectorizer): \n",
    "    \"\"\"\n",
    "    Encodes a text column with the given vectorizer, drops the old column (with text),\n",
    "    and returns the dataframe with the encoded text.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe.\n",
    "        text_column (str): The name of the text column to be encoded.\n",
    "        vectorizer: The text vectorizer.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with the encoded text.\n",
    "    \"\"\"\n",
    "    # Replace NaN values with an empty string\n",
    "    df[text_column] = df[text_column].fillna('')\n",
    "\n",
    "    # Vectorize the text column\n",
    "    df_vectorized = vectorizer.fit_transform(df[text_column])\n",
    "    \n",
    "    # Create a dataframe from the vectorized data\n",
    "    df_encoded = pd.DataFrame(df_vectorized.toarray(), columns=[f\"{text_column}_{i}\" for i in range(df_vectorized.shape[1])])\n",
    "\n",
    "    # Drop the original text column\n",
    "    df.drop([text_column], axis=1, inplace=True)\n",
    "\n",
    "    # Concatenate the original dataframe with the encoded text dataframe\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "    # Ensure all column names are strings\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modelready_220423.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_encode(df):\n",
    "    # Extract unique countries in the df\n",
    "    unique_values = set()\n",
    "    df['countries_in_family'].apply(lambda x: unique_values.update(x.strip(\"[]\").replace(\"'\", \"\").split())) \n",
    "\n",
    "    # Create new columns for each unique value\n",
    "    for value in unique_values:\n",
    "        # each country has a column (1 if the patent belongs to the country 0 otherwise)\n",
    "        df[value] = df['countries_in_family'].apply(lambda x: 1 if value in x else 0)\n",
    "\n",
    "    # Drop samples without abstract\n",
    "    df = df[df.abstract.notna()].copy()\n",
    "\n",
    "    # Encode company names\n",
    "    df['company_name_encoded'] = df.company_name.astype('category').cat.codes  \n",
    "\n",
    "    # Remove non-numeric columns\n",
    "    df_columns_dropped = df.drop(['publication_number', 'company_name', 'countries_in_family', 'publn_nr','primary_cpc'], axis=1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_columns_dropped = df_columns_dropped.drop(['f0_', 'centrality', 'similarity'], axis=1)\n",
    "\n",
    "    # Drop text columns temporarily\n",
    "    text = df_columns_dropped[['abstract', 'description_text']]\n",
    "    df_columns_dropped.drop(['abstract', 'description_text'], axis=1, inplace=True)\n",
    "\n",
    "    # Drop columns where all values are the same\n",
    "    min_eq_max = df_columns_dropped.columns[df_columns_dropped.min() == df_columns_dropped.max()].to_list()\n",
    "    df_clean = df_columns_dropped.drop(min_eq_max, axis=1)\n",
    "\n",
    "\n",
    "    # Impute NaN values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_no_missing = pd.DataFrame(imputer.fit_transform(df_clean), columns=df_clean.columns)\n",
    "    \n",
    "    # Extract target variable\n",
    "    y = df_no_missing['commercialized']\n",
    "    df_no_missing.drop('commercialized', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Rescale\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Putting text back in\n",
    "    df_clean[['abstract', 'description_text']] = text  \n",
    "\n",
    "    # Split again after putting text back in\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Same vectorizer applied to training and testing for abstract\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X_train_ab = encode_text_column(X_train, 'abstract', vectorizer)\n",
    "    X_test_ab = encode_text_column(X_test, 'abstract', vectorizer)\n",
    "\n",
    "    # Same vectorizer applied to training and testing for description_text\n",
    "    X_train_de = encode_text_column(X_train_ab, 'description_text', vectorizer)\n",
    "    X_test_de = encode_text_column(X_test_ab, 'description_text', vectorizer)\n",
    "\n",
    "    return X_train_de, X_test_de, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X_train, X_test, y_train, y_test):\n",
    "    # Train a Multinomial Naive Bayes classifier\n",
    "    naive_bayes_model = MultinomialNB()\n",
    "    naive_bayes_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = naive_bayes_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report_str)\n",
    "\n",
    "    return naive_bayes_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nMultinomialNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train_de, X_test_de, y_train, y_test \u001b[38;5;241m=\u001b[39m preprocess_and_encode(df)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train and evaluate the Naive Bayes model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m naive_bayes_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_naive_bayes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mtrain_naive_bayes\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_naive_bayes\u001b[39m(X_train, X_test, y_train, y_test):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Train a Multinomial Naive Bayes classifier\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     naive_bayes_model \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mnaive_bayes_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m naive_bayes_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/naive_bayes.py:745\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    726\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \n\u001b[1;32m    728\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[1;32m    746\u001b[0m     _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    748\u001b[0m     labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/naive_bayes.py:578\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X_y\u001b[39m(\u001b[39mself\u001b[39m, X, y, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    577\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49mreset)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[1;32m    958\u001b[0m             array,\n\u001b[1;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    123\u001b[0m     X,\n\u001b[1;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nMultinomialNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing and encoding function\n",
    "X_train_de, X_test_de, y_train, y_test = preprocess_and_encode(df)\n",
    "\n",
    "# Train and evaluate the Naive Bayes model\n",
    "naive_bayes_model = train_naive_bayes(X_train_de, X_test_de, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
