{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "(This is the working notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Extract unique countries in the df\n",
    "    unique_values = set()\n",
    "    df['countries_in_family'].apply(lambda x: unique_values.update(x.strip(\"[]\").replace(\"'\", \"\").split())) \n",
    "\n",
    "    # Create new columns for each unique value\n",
    "    for value in unique_values:\n",
    "        # each country has a column (1 if the patent belongs to the country 0 otherwise)\n",
    "        df[value] = df['countries_in_family'].apply(lambda x: 1 if value in x else 0)\n",
    "\n",
    "    df = df[df.abstract.notna()].copy()  # drop all samples without abstract\n",
    "\n",
    "    # Encode company names\n",
    "    df['company_name_encoded'] = df.company_name.astype('category').cat.codes  # encode companies\n",
    "\n",
    "    # Remove non-numeric columns\n",
    "    df_columns_dropped = df.drop(['publication_number', 'company_name', 'countries_in_family', 'publn_nr','primary_cpc'], axis=1)\n",
    "\n",
    "    # f0_ has the same value as commercialization, the other two shouldn't be used\n",
    "    df_columns_dropped = df_columns_dropped.drop(['f0_', 'centrality', 'similarity'], axis=1)\n",
    "\n",
    "    # Remove text as I can't compute min and max on it\n",
    "    text = df_columns_dropped[['abstract', 'description_text']]  # putting them aside for later\n",
    "    df_columns_dropped.drop(['abstract', 'description_text'], axis=1, inplace=True)\n",
    "\n",
    "    df_no_missing = df_columns_dropped.fillna(df_columns_dropped.mean()).copy()\n",
    "\n",
    "    # Extracting what we'll try to predict\n",
    "    y = df_no_missing['commercialized']\n",
    "    df_no_missing.drop('commercialized', axis=1, inplace=True)\n",
    "\n",
    "    # Dropping columns where all the values are the same (min = max)\n",
    "    min_eq_max = df_no_missing.columns[df_no_missing.min() == df_no_missing.max()].to_list()\n",
    "    df_clean = df_no_missing.drop(min_eq_max, axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Rescale\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # # Putting text back in\n",
    "    # df_clean[['abstract', 'description_text']] = text \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42) \n",
    "\n",
    "    # # Same vectorizer applied to training and testing for abstract\n",
    "    # vectorizer = TfidfVectorizer(max_features=1000)  # Adjust 'max_features' as needed\n",
    "    # X_train_ab = encode_text_colum(X_train, 'abstract', vectorizer)\n",
    "    # X_test_ab = encode_text_colum(X_test, 'abstract', vectorizer)\n",
    "\n",
    "    # # Same vectorizer applied to training and testing for description_text\n",
    "    # vectorizer = TfidfVectorizer(max_features=1000)  # Adjust 'max_features' as needed\n",
    "    # X_train_de = encode_text_colum(X_train_ab, 'description_text', vectorizer)\n",
    "    # X_test_de = encode_text_colum(X_test_ab, 'description_text', vectorizer)\n",
    "\n",
    "    # return X_train_de, X_test_de, y_train, y_test\n",
    "    return df_clean, y, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modelready_220423.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply the preprocessing and encoding function\n",
    "df_clean, y, X_train, X_test, y_train, y_test = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There aren't any NaN values in X_train_de:\n",
      ">> True\n"
     ]
    }
   ],
   "source": [
    "print(f\"There aren't any NaN values in X_train_de:\\n>> {X_train.isna().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valerio/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/valerio/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8186311077955987\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def train_naive_bayes_model(X_train, y_train):\n",
    "    # Create and fit the Naive Bayes model\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    return nb_classifier\n",
    "\n",
    "def evaluate_model(nb_classifier, X_test, y_test):\n",
    "    # Make predictions using the trained model\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Rescaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_array = scaler.transform(X_train.values)\n",
    "X_test_array = scaler.transform(X_test.values)\n",
    "\n",
    "# Load the preprocessed data\n",
    "X_train = X_train_array\n",
    "X_test = X_test_array\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb_classifier = train_naive_bayes_model(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance\n",
    "evaluate_model(nb_classifier, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_df(df, cols_to_drop):\n",
    "    df_out = df.copy()\n",
    "    df_out = df_out.drop(cols_to_drop, axis=1)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN values in the DataFrame: False\n"
     ]
    }
   ],
   "source": [
    "any_nan = df_clean.isna().any().any()\n",
    "print(\"Any NaN values in the DataFrame:\", any_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        140\n",
       "1        203\n",
       "2         69\n",
       "3        162\n",
       "4        208\n",
       "        ... \n",
       "63342     75\n",
       "63343     15\n",
       "63346     64\n",
       "63347      1\n",
       "63348     50\n",
       "Name: backward_citations_app, Length: 53616, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['backward_citations_app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53616, 842)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = ['backward_citations_app', 'backward_citations_exa'] \n",
    "modify_df(df_clean, to_drop).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gt/blghfghs40xbwq3vs62clyyh0000gn/T/ipykernel_66662/4235620497.py:48: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m         evaluate_model(nb_classifier, X_test_mod, y_test)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Load your preprocessed data and define feature groups\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Replace df_clean with the actual variable containing your preprocessed data\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m feature_groups \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Assuming X_train_array, X_test_array, y_train, y_test are defined\u001b[39;00m\n\u001b[1;32m    107\u001b[0m iterate_feature_groups(X_train_array, X_test_array, y_train, y_test, feature_groups)\n",
      "Cell \u001b[0;32mIn[48], line 48\u001b[0m, in \u001b[0;36mgroup_features\u001b[0;34m(df_clean, threshold)\u001b[0m\n\u001b[1;32m     45\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m df_clean\u001b[38;5;241m.\u001b[39mcorr()\u001b[38;5;241m.\u001b[39mabs()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Extract the upper triangle of the correlation matrix\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m upper_triangle \u001b[38;5;241m=\u001b[39m corr_matrix\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mtriu(np\u001b[38;5;241m.\u001b[39mones(corr_matrix\u001b[38;5;241m.\u001b[39mshape), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Find features with correlation above the threshold\u001b[39;00m\n\u001b[1;32m     51\u001b[0m correlated_features \u001b[38;5;241m=\u001b[39m [column \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m upper_triangle\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(upper_triangle[column] \u001b[38;5;241m>\u001b[39m threshold)]\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load your preprocessed data (replace X_train_array, X_test_array, y_train, y_test with your actual data)\n",
    "# # For this example, I'll assume you have them already defined.\n",
    "# # X_train_array, X_test_array, y_train, y_test = ...\n",
    "\n",
    "# # Function to train Naive Bayes model\n",
    "# def train_naive_bayes_model(X_train, y_train):\n",
    "#     nb_classifier = MultinomialNB()\n",
    "#     nb_classifier.fit(X_train, y_train)\n",
    "#     return nb_classifier\n",
    "\n",
    "# # Function to evaluate Naive Bayes model\n",
    "# def evaluate_model(nb_classifier, X_test, y_test):\n",
    "#     y_pred = nb_classifier.predict(X_test)\n",
    "#     accuracy = np.mean(y_pred == y_test)\n",
    "#     print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # Function to modify the dataframe by dropping specified columns\n",
    "# def modify_df(df, cols_to_drop):\n",
    "#     df_out = df.drop(cols_to_drop, axis=1)\n",
    "#     return df_out\n",
    "\n",
    "# # Step 1: Group Features Based on Correlation\n",
    "# correlation_matrix = df_clean.corr().abs()\n",
    "\n",
    "# # Set a correlation threshold\n",
    "# correlation_threshold = 0.8\n",
    "\n",
    "# # Find groups of correlated features\n",
    "# feature_groups = []\n",
    "# while len(correlation_matrix.columns) > 0:\n",
    "#     feature = correlation_matrix.columns[0]\n",
    "#     correlated_features = list(correlation_matrix[feature][correlation_matrix[feature] > correlation_threshold].index)\n",
    "#     feature_groups.append([feature] + correlated_features)\n",
    "#     correlation_matrix = correlation_matrix.drop(correlated_features, axis=1)\n",
    "#     correlation_matrix = correlation_matrix.drop(feature, axis=0)\n",
    "\n",
    "# # Step 2: Iterate Over Feature Groups and Train Naive Bayes\n",
    "# def group_features(df_clean, threshold=0.9):\n",
    "#     # Compute the correlation matrix\n",
    "#     corr_matrix = df_clean.corr().abs()\n",
    "\n",
    "#     # Extract the upper triangle of the correlation matrix\n",
    "#     upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "#     # Find features with correlation above the threshold\n",
    "#     correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "\n",
    "#     # Create a list of groups of features\n",
    "#     grouped_features = []\n",
    "#     for feature in correlated_features:\n",
    "#         # Check if the feature is already in a group\n",
    "#         added = False\n",
    "#         for group in grouped_features:\n",
    "#             if feature in group:\n",
    "#                 added = True\n",
    "#                 break\n",
    "\n",
    "#         # If the feature is not in any group, create a new group\n",
    "#         if not added:\n",
    "#             new_group = [feature]\n",
    "#             # Add correlated features to the group\n",
    "#             for correlated_feature in correlated_features:\n",
    "#                 if correlated_feature != feature and corr_matrix.loc[feature, correlated_feature] > threshold:\n",
    "#                     new_group.append(correlated_feature)\n",
    "#             grouped_features.append(new_group)\n",
    "\n",
    "#     return grouped_features\n",
    "\n",
    "\n",
    "\n",
    "# # Function to iterate over feature groups, modify dataframe, and print outcome\n",
    "# def iterate_feature_groups(X_train, X_test, y_train, y_test, feature_groups):\n",
    "#     for group in feature_groups:\n",
    "#         print(f\"Performance after removing features {group}:\")\n",
    "\n",
    "#         # Modify the dataframes by dropping the current group of features\n",
    "#         X_train_mod = modify_df(X_train, group)\n",
    "#         X_test_mod = modify_df(X_test, group)\n",
    "\n",
    "#         # Rescaling\n",
    "#         scaler = MinMaxScaler()\n",
    "#         scaler.fit(X_train)\n",
    "#         X_train_array_mod = scaler.transform(X_train_mod.values)\n",
    "#         X_test_array_mod = scaler.transform(X_test_mod.values)\n",
    "\n",
    "#         # Load the preprocessed data\n",
    "#         X_train_mod = X_train_array_mod\n",
    "#         X_test_mod = X_test_array_mod\n",
    "\n",
    "#         # Train Naive Bayes model on modified data\n",
    "#         nb_classifier = train_naive_bayes_model(X_train_mod, y_train)\n",
    "\n",
    "#         # Evaluate the model on the test set\n",
    "#         evaluate_model(nb_classifier, X_test_mod, y_test)\n",
    "\n",
    "# # Load your preprocessed data and define feature groups\n",
    "# # Replace df_clean with the actual variable containing your preprocessed data\n",
    "# feature_groups = group_features(df_clean, threshold=0.8)\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming X_train_array, X_test_array, y_train, y_test are defined\n",
    "# iterate_feature_groups(X_train_array, X_test_array, y_train, y_test, feature_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gt/blghfghs40xbwq3vs62clyyh0000gn/T/ipykernel_66662/2648778101.py:33: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 86\u001b[0m\n\u001b[1;32m     82\u001b[0m         evaluate_model(nb_classifier, X_test_mod, y_test)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Load your preprocessed data and define feature groups\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Replace df_clean with the actual variable containing your preprocessed data\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m feature_groups \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Assuming X_train_array, X_test_array, y_train, y_test are defined\u001b[39;00m\n\u001b[1;32m     90\u001b[0m iterate_feature_groups(X_train_array, X_test_array, y_train, y_test, feature_groups)\n",
      "Cell \u001b[0;32mIn[47], line 33\u001b[0m, in \u001b[0;36mgroup_features\u001b[0;34m(df_clean, threshold)\u001b[0m\n\u001b[1;32m     30\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m df_clean\u001b[38;5;241m.\u001b[39mcorr()\u001b[38;5;241m.\u001b[39mabs()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Extract the upper triangle of the correlation matrix\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m upper_triangle \u001b[38;5;241m=\u001b[39m corr_matrix\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mtriu(np\u001b[38;5;241m.\u001b[39mones(corr_matrix\u001b[38;5;241m.\u001b[39mshape), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Find features with correlation above the threshold\u001b[39;00m\n\u001b[1;32m     36\u001b[0m correlated_features \u001b[38;5;241m=\u001b[39m [column \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m upper_triangle\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(upper_triangle[column] \u001b[38;5;241m>\u001b[39m threshold)]\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your preprocessed data\n",
    "# For this example, I'll assume you have them already defined.\n",
    "# X_train_array, X_test_array, y_train, y_test = ...\n",
    "\n",
    "# Function to train Naive Bayes model\n",
    "def train_naive_bayes_model(X_train, y_train):\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    return nb_classifier\n",
    "\n",
    "# Function to evaluate Naive Bayes model\n",
    "def evaluate_model(nb_classifier, X_test, y_test):\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Function to modify the dataframe by dropping specified columns\n",
    "def modify_df(df, cols_to_drop):\n",
    "    df_out = df.drop(cols_to_drop, axis=1)\n",
    "    return df_out\n",
    "\n",
    "# Iterate Over Feature Groups and Train Naive Bayes\n",
    "def group_features(df_clean, threshold=0.8):\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = df_clean.corr().abs()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find features with correlation above the threshold\n",
    "    correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "\n",
    "    # Create a list of groups of features\n",
    "    grouped_features = []\n",
    "    for feature in correlated_features:\n",
    "        # Check if the feature is already in a group\n",
    "        added = False\n",
    "        for group in grouped_features:\n",
    "            if feature in group:\n",
    "                added = True\n",
    "                break\n",
    "\n",
    "        # If the feature is not in any group, create a new group\n",
    "        if not added:\n",
    "            new_group = [feature]\n",
    "            # Add correlated features to the group\n",
    "            for correlated_feature in correlated_features:\n",
    "                if correlated_feature != feature and corr_matrix.loc[feature, correlated_feature] > threshold:\n",
    "                    new_group.append(correlated_feature)\n",
    "            grouped_features.append(new_group)\n",
    "\n",
    "    return grouped_features\n",
    "\n",
    "# Function to iterate over feature groups, modify dataframe, and print outcome\n",
    "def iterate_feature_groups(X_train, X_test, y_train, y_test, feature_groups):\n",
    "    for group in feature_groups:\n",
    "        print(f\"Performance after removing features {group}:\")\n",
    "\n",
    "        # Modify the dataframes by dropping the current group of features\n",
    "        X_train_mod = modify_df(X_train, group)\n",
    "        X_test_mod = modify_df(X_test, group)\n",
    "\n",
    "        # Rescaling\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_array_mod = scaler.transform(X_train_mod.values)\n",
    "        X_test_array_mod = scaler.transform(X_test_mod.values)\n",
    "\n",
    "        # Load the preprocessed data\n",
    "        X_train_mod = X_train_array_mod\n",
    "        X_test_mod = X_test_array_mod\n",
    "\n",
    "        # Train Naive Bayes model on modified data\n",
    "        nb_classifier = train_naive_bayes_model(X_train_mod, y_train)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        evaluate_model(nb_classifier, X_test_mod, y_test)\n",
    "\n",
    "# Load your preprocessed data and define feature groups\n",
    "# Replace df_clean with the actual variable containing your preprocessed data\n",
    "feature_groups = group_features(df_clean, threshold=0.8)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train_array, X_test_array, y_train, y_test are defined\n",
    "iterate_feature_groups(X_train_array, X_test_array, y_train, y_test, feature_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
