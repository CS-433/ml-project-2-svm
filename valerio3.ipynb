{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "(This is the working notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Extract unique countries in the df\n",
    "    unique_values = set()\n",
    "    df['countries_in_family'].apply(lambda x: unique_values.update(x.strip(\"[]\").replace(\"'\", \"\").split())) \n",
    "\n",
    "    # Create new columns for each unique value\n",
    "    for value in unique_values:\n",
    "        # each country has a column (1 if the patent belongs to the country 0 otherwise)\n",
    "        df[value] = df['countries_in_family'].apply(lambda x: 1 if value in x else 0)\n",
    "\n",
    "    df = df[df.abstract.notna()].copy()  # drop all samples without abstract\n",
    "\n",
    "    # Encode company names\n",
    "    df['company_name_encoded'] = df.company_name.astype('category').cat.codes  # encode companies\n",
    "\n",
    "    # Remove non-numeric columns\n",
    "    df_columns_dropped = df.drop(['publication_number', 'company_name', 'countries_in_family', 'publn_nr','primary_cpc'], axis=1)\n",
    "\n",
    "    # f0_ has the same value as commercialization, the other two shouldn't be used\n",
    "    df_columns_dropped = df_columns_dropped.drop(['f0_', 'centrality', 'similarity'], axis=1)\n",
    "\n",
    "    # Remove text as I can't compute min and max on it\n",
    "    text = df_columns_dropped[['abstract', 'description_text']]  # putting them aside for later\n",
    "    df_columns_dropped.drop(['abstract', 'description_text'], axis=1, inplace=True)\n",
    "\n",
    "    df_no_missing = df_columns_dropped.fillna(df_columns_dropped.mean()).copy()\n",
    "\n",
    "    # Extracting what we'll try to predict\n",
    "    y = df_no_missing['commercialized']\n",
    "    df_no_missing.drop('commercialized', axis=1, inplace=True)\n",
    "\n",
    "    # Dropping columns where all the values are the same (min = max)\n",
    "    min_eq_max = df_no_missing.columns[df_no_missing.min() == df_no_missing.max()].to_list()\n",
    "    df_clean = df_no_missing.drop(min_eq_max, axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Rescale\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # # Putting text back in\n",
    "    # df_clean[['abstract', 'description_text']] = text \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42) \n",
    "\n",
    "    # # Same vectorizer applied to training and testing for abstract\n",
    "    # vectorizer = TfidfVectorizer(max_features=1000)  # Adjust 'max_features' as needed\n",
    "    # X_train_ab = encode_text_colum(X_train, 'abstract', vectorizer)\n",
    "    # X_test_ab = encode_text_colum(X_test, 'abstract', vectorizer)\n",
    "\n",
    "    # # Same vectorizer applied to training and testing for description_text\n",
    "    # vectorizer = TfidfVectorizer(max_features=1000)  # Adjust 'max_features' as needed\n",
    "    # X_train_de = encode_text_colum(X_train_ab, 'description_text', vectorizer)\n",
    "    # X_test_de = encode_text_colum(X_test_ab, 'description_text', vectorizer)\n",
    "\n",
    "    # return X_train_de, X_test_de, y_train, y_test\n",
    "    return df_clean, y, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modelready_220423.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply the preprocessing and encoding function\n",
    "df_clean, y, X_train, X_test, y_train, y_test = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There aren't any NaN values in X_train_de:\n",
      ">> True\n"
     ]
    }
   ],
   "source": [
    "print(f\"There aren't any NaN values in X_train_de:\\n>> {X_train.isna().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8186311077955987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valerio/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/valerio/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def train_naive_bayes_model(X_train, y_train):\n",
    "    # Create and fit the Naive Bayes model\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    return nb_classifier\n",
    "\n",
    "def evaluate_model(nb_classifier, X_test, y_test):\n",
    "    # Make predictions using the trained model\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Rescaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_array = scaler.transform(X_train.values)\n",
    "X_test_array = scaler.transform(X_test.values)\n",
    "\n",
    "# Load the preprocessed data\n",
    "X_train = X_train_array\n",
    "X_test = X_test_array\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb_classifier = train_naive_bayes_model(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance\n",
    "evaluate_model(nb_classifier, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_df(df, cols_to_drop):\n",
    "    df_out = df.copy()\n",
    "    df_out = df_out.drop(cols_to_drop, axis=1)\n",
    "    return df_out\n",
    "\n",
    "# def modify_df(arr, cols_to_drop):\n",
    "#     arr_out = np.delete(arr, np.where(np.isin(arr.columns, cols_to_drop)), axis=1)\n",
    "#     return arr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN values in the DataFrame: False\n"
     ]
    }
   ],
   "source": [
    "any_nan = df_clean.isna().any().any()\n",
    "print(\"Any NaN values in the DataFrame:\", any_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        140\n",
       "1        203\n",
       "2         69\n",
       "3        162\n",
       "4        208\n",
       "        ... \n",
       "63342     75\n",
       "63343     15\n",
       "63346     64\n",
       "63347      1\n",
       "63348     50\n",
       "Name: backward_citations_app, Length: 53616, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['backward_citations_app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53616, 842)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = ['backward_citations_app', 'backward_citations_exa'] \n",
    "modify_df(df_clean, to_drop).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed data\n",
    "df_clean, y, X_train, X_test, y_train, y_test = preprocess(df)\n",
    "\n",
    "# Function to train Naive Bayes model\n",
    "def train_naive_bayes_model(X_train, y_train):\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    return nb_classifier\n",
    "\n",
    "# Function to evaluate Naive Bayes model\n",
    "def evaluate_model(nb_classifier, X_test, y_test):\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # Function to modify the dataframe by dropping specified columns\n",
    "# def modify_df(df, cols_to_drop):\n",
    "#     df_out = df.copy()\n",
    "#     df_out = df_out.drop(cols_to_drop, axis=1)\n",
    "#     return df_out\n",
    "\n",
    "# Function to modify the dataframe by dropping specified columns\n",
    "def modify_df(df, cols_to_drop):\n",
    "    df_out = df.copy()\n",
    "    df_out = df_out.drop(cols_to_drop, axis=1, errors='ignore')  # Add errors='ignore' to handle non-existing columns\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# Iterate Over Feature Groups and Train Naive Bayes\n",
    "def group_features(df_clean, threshold=0.8):\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = df_clean.corr().abs()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find features with correlation above the threshold\n",
    "    correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "\n",
    "    # Create a list of groups of features with at least two variables\n",
    "    grouped_features = []\n",
    "    for feature in correlated_features:\n",
    "        # Check if the feature is already in a group\n",
    "        added = False\n",
    "        for group in grouped_features:\n",
    "            if feature in group:\n",
    "                added = True\n",
    "                break\n",
    "\n",
    "        # If the feature is not in any group, create a new group\n",
    "        if not added:\n",
    "            new_group = [feature]\n",
    "            # Add correlated features to the group\n",
    "            for correlated_feature in correlated_features:\n",
    "                if correlated_feature != feature and corr_matrix.loc[feature, correlated_feature] > threshold:\n",
    "                    new_group.append(correlated_feature)\n",
    "            # Only add the group if it has at least two variables\n",
    "            if len(new_group) > 1:\n",
    "                grouped_features.append(new_group)\n",
    "\n",
    "    return grouped_features\n",
    "\n",
    "# Function to iterate over feature groups, modify dataframe, and print outcome\n",
    "def iterate_feature_groups(X_train, X_test, y_train, y_test, feature_groups):\n",
    "    for group in feature_groups:\n",
    "        print(f\"Performance after removing features {group}:\")\n",
    "\n",
    "        # Modify the dataframes by dropping the current group of features\n",
    "        X_train_mod = modify_df(pd.DataFrame(X_train), group)\n",
    "        X_test_mod = modify_df(pd.DataFrame(X_test), group)\n",
    "\n",
    "        # Rescaling\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_array_mod = scaler.transform(X_train_mod.values)\n",
    "        X_test_array_mod = scaler.transform(X_test_mod.values)\n",
    "\n",
    "        # Load the preprocessed data\n",
    "        X_train_mod = X_train_array_mod\n",
    "        X_test_mod = X_test_array_mod\n",
    "\n",
    "        # Train Naive Bayes model on modified data\n",
    "        nb_classifier = train_naive_bayes_model(X_train_mod, y_train)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        evaluate_model(nb_classifier, X_test_mod, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test n.1\n",
    "threshold = 0.8\n",
    "\n",
    "(threshold indicates how correlated the features must be in order to group them together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance after removing features ['dummy_country_ZW', 'MD', 'ZW']:\n",
      "Accuracy: 0.8186311077955987\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data and define feature groups\n",
    "feature_groups = group_features(df_clean, threshold=0.8)\n",
    "\n",
    "# Usage:\n",
    "iterate_feature_groups(X_train_array, X_test_array, y_train, y_test, feature_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test n.2\n",
    "threshold = 0.5\n",
    "\n",
    "(threshold indicates how correlated the features must be in order to group them together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance after removing features ['geog_family_size_x', 'geog_family_size_y', 'dummy_country_AU', 'dummy_country_BR', 'dummy_country_CA', 'dummy_country_CN', 'dummy_country_DK', 'dummy_country_EP', 'dummy_country_ES', 'dummy_country_IL', 'dummy_country_JP', 'dummy_country_KR', 'dummy_country_MX', 'dummy_country_PL', 'dummy_country_PT', 'MX', 'KR', 'PL', 'BR', 'EP', 'CN', 'IL', 'CA', 'JP', 'AU', 'PT', 'DK', 'ES']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_CR', 'geog_family_size_y', 'dummy_country_EC', 'dummy_country_PE', 'CR', 'PE', 'EC']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_CY', 'geog_family_size_y', 'dummy_country_SI', 'CY', 'SI']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_CZ', 'dummy_country_SK', 'SK', 'CZ', 'BG']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_DE', 'AT', 'DE']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_EA', 'geog_family_size_y', 'EA']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_GT', 'dummy_country_HN', 'dummy_country_PA', 'HN', 'PA', 'AP', 'GT']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_HR', 'geog_family_size_y', 'dummy_country_HU', 'dummy_country_LT', 'dummy_country_RS', 'dummy_country_SI', 'LT', 'SI', 'RS', 'HU', 'HR']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_IS', 'dummy_country_MA', 'dummy_country_OA', 'OA', 'MA', 'AP', 'IS']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_NL', 'LU', 'NL']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_NZ', 'geog_family_size_y', 'NZ']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_RU', 'dummy_country_BR', 'RU', 'BR']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_TN', 'geog_family_size_y', 'dummy_country_MA', 'dummy_country_PA', 'MA', 'PA', 'AP', 'TN']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_UA', 'geog_family_size_y', 'UA']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_ZA', 'geog_family_size_y', 'ZA']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_country_ZW', 'MD', 'ZW']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['grant_year', 'filing_year']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_cpc_C05C', 'dummy_cpc_C05D', 'dummy_cpc_C05G']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_cpc_C08H', 'dummy_cpc_C13K']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_cpc_D02G', 'dummy_cpc_D02J']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_cpc_D03D', 'dummy_cpc_D03J', 'dummy_cpc_D06C']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_cpc_E05G', 'dummy_cpc_F41H']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_cpc_F21V', 'dummy_cpc_F21Y']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['dummy_cpc_F23D', 'dummy_cpc_F23M']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['GE', 'dummy_country_MA', 'MA']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['DZ', 'dummy_country_OA', 'OA']:\n",
      "Accuracy: 0.8186311077955987\n",
      "Performance after removing features ['ME', 'dummy_country_RS', 'RS']:\n",
      "Accuracy: 0.8186311077955987\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data and define feature groups\n",
    "feature_groups = group_features(df_clean, threshold=0.5)\n",
    "\n",
    "# Usage:\n",
    "iterate_feature_groups(X_train_array, X_test_array, y_train, y_test, feature_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geog_family_size_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63342</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63343</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63346</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63347</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63348</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53616 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geog_family_size_y\n",
       "0                      16\n",
       "1                       1\n",
       "2                       1\n",
       "3                      64\n",
       "4                      16\n",
       "...                   ...\n",
       "63342                  16\n",
       "63343                   4\n",
       "63346                   1\n",
       "63347                   4\n",
       "63348                   4\n",
       "\n",
       "[53616 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[['geog_family_size_y']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
