{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "(This is the working notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Extract unique countries in the df\n",
    "    unique_values = set()\n",
    "    df['countries_in_family'].apply(lambda x: unique_values.update(x.strip(\"[]\").replace(\"'\", \"\").split())) \n",
    "\n",
    "    # Create new columns for each unique value\n",
    "    for value in unique_values:\n",
    "        # each country has a column (1 if the patent belongs to the country 0 otherwise)\n",
    "        df[value] = df['countries_in_family'].apply(lambda x: 1 if value in x else 0)\n",
    "\n",
    "    df = df[df.abstract.notna()].copy()  # drop all samples without abstract\n",
    "\n",
    "    # Encode company names\n",
    "    df['company_name_encoded'] = df.company_name.astype('category').cat.codes  # encode companies\n",
    "\n",
    "    # Remove non-numeric columns\n",
    "    df_columns_dropped = df.drop(['publication_number', 'company_name', 'countries_in_family', 'publn_nr','primary_cpc'], axis=1)\n",
    "\n",
    "    # f0_ has the same value as commercialization, the other two shouldn't be used\n",
    "    df_columns_dropped = df_columns_dropped.drop(['f0_', 'centrality', 'similarity'], axis=1)\n",
    "\n",
    "    # Remove text as I can't compute min and max on it\n",
    "    text = df_columns_dropped[['abstract', 'description_text']]  # putting them aside for later\n",
    "    df_columns_dropped.drop(['abstract', 'description_text'], axis=1, inplace=True)\n",
    "\n",
    "    df_no_missing = df_columns_dropped.fillna(df_columns_dropped.mean()).copy()\n",
    "\n",
    "    # Extracting what we'll try to predict\n",
    "    y = df_no_missing['commercialized']\n",
    "    df_no_missing.drop('commercialized', axis=1, inplace=True)\n",
    "\n",
    "    # Dropping columns where all the values are the same (min = max)\n",
    "    min_eq_max = df_no_missing.columns[df_no_missing.min() == df_no_missing.max()].to_list()\n",
    "    df_clean = df_no_missing.drop(min_eq_max, axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Rescale\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # # Putting text back in\n",
    "    # df_clean[['abstract', 'description_text']] = text \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_clean, y, test_size=0.20, random_state=42) \n",
    "\n",
    "    # # Same vectorizer applied to training and testing for abstract\n",
    "    # vectorizer = TfidfVectorizer(max_features=1000)  # Adjust 'max_features' as needed\n",
    "    # X_train_ab = encode_text_colum(X_train, 'abstract', vectorizer)\n",
    "    # X_test_ab = encode_text_colum(X_test, 'abstract', vectorizer)\n",
    "\n",
    "    # # Same vectorizer applied to training and testing for description_text\n",
    "    # vectorizer = TfidfVectorizer(max_features=1000)  # Adjust 'max_features' as needed\n",
    "    # X_train_de = encode_text_colum(X_train_ab, 'description_text', vectorizer)\n",
    "    # X_test_de = encode_text_colum(X_test_ab, 'description_text', vectorizer)\n",
    "\n",
    "    # return X_train_de, X_test_de, y_train, y_test\n",
    "    return df_clean, y, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modelready_220423.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply the preprocessing and encoding function\n",
    "df_clean, y, X_train, X_test, y_train, y_test = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There aren't any NaN values in X_train_de:\n",
      ">> True\n"
     ]
    }
   ],
   "source": [
    "print(f\"There aren't any NaN values in X_train_de:\\n>> {X_train.isna().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8186311077955987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valerio/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/valerio/Library/Python/3.11/lib/python/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def train_naive_bayes_model(X_train, y_train):\n",
    "    # Create and fit the Naive Bayes model\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    return nb_classifier\n",
    "\n",
    "def evaluate_model(nb_classifier, X_test, y_test):\n",
    "    # Make predictions using the trained model\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Rescaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_array = scaler.transform(X_train.values)\n",
    "X_test_array = scaler.transform(X_test.values)\n",
    "\n",
    "# Load the preprocessed data\n",
    "X_train = X_train_array\n",
    "X_test = X_test_array\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb_classifier = train_naive_bayes_model(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance\n",
    "evaluate_model(nb_classifier, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_df(df, cols_to_drop):\n",
    "    df_out = df.copy()\n",
    "    df_out = df_out.drop(cols_to_drop, axis=1)\n",
    "    return df_out\n",
    "\n",
    "# def modify_df(arr, cols_to_drop):\n",
    "#     arr_out = np.delete(arr, np.where(np.isin(arr.columns, cols_to_drop)), axis=1)\n",
    "#     return arr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN values in the DataFrame: False\n"
     ]
    }
   ],
   "source": [
    "any_nan = df_clean.isna().any().any()\n",
    "print(\"Any NaN values in the DataFrame:\", any_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        140\n",
       "1        203\n",
       "2         69\n",
       "3        162\n",
       "4        208\n",
       "        ... \n",
       "63342     75\n",
       "63343     15\n",
       "63346     64\n",
       "63347      1\n",
       "63348     50\n",
       "Name: backward_citations_app, Length: 53616, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['backward_citations_app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53616, 842)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = ['backward_citations_app', 'backward_citations_exa'] \n",
    "modify_df(df_clean, to_drop).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance after removing features ['geog_family_size_y']:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['geog_family_size_y'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m feature_groups \u001b[38;5;241m=\u001b[39m group_features(df_clean, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Assuming X_train_array, X_test_array, y_train, y_test are defined\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[43miterate_feature_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_groups\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 66\u001b[0m, in \u001b[0;36miterate_feature_groups\u001b[0;34m(X_train, X_test, y_train, y_test, feature_groups)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformance after removing features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Modify the dataframes by dropping the current group of features\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m X_train_mod \u001b[38;5;241m=\u001b[39m \u001b[43mmodify_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m X_test_mod \u001b[38;5;241m=\u001b[39m modify_df(pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test), group)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Rescaling\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 24\u001b[0m, in \u001b[0;36mmodify_df\u001b[0;34m(df, cols_to_drop)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify_df\u001b[39m(df, cols_to_drop):\n\u001b[1;32m     23\u001b[0m     df_out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 24\u001b[0m     df_out \u001b[38;5;241m=\u001b[39m \u001b[43mdf_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols_to_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_out\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[1;32m   5197\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   5198\u001b[0m     labels: IndexLabel \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5206\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5208\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5342\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5343\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5345\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5346\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5347\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5348\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5349\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5350\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5351\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5352\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4710\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4711\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4713\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4714\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4752\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4754\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4756\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/indexes/base.py:6992\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6990\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6991\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6992\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6993\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   6994\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['geog_family_size_y'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your preprocessed data\n",
    "df_clean, y, X_train, X_test, y_train, y_test = preprocess(df)\n",
    "\n",
    "# Function to train Naive Bayes model\n",
    "def train_naive_bayes_model(X_train, y_train):\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    return nb_classifier\n",
    "\n",
    "# Function to evaluate Naive Bayes model\n",
    "def evaluate_model(nb_classifier, X_test, y_test):\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Function to modify the dataframe by dropping specified columns\n",
    "def modify_df(df, cols_to_drop):\n",
    "    df_out = df.copy()\n",
    "    df_out = df_out.drop(cols_to_drop, axis=1)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# Iterate Over Feature Groups and Train Naive Bayes\n",
    "def group_features(df_clean, threshold=0.8):\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = df_clean.corr().abs()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find features with correlation above the threshold\n",
    "    correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "\n",
    "    # Create a list of groups of features\n",
    "    grouped_features = []\n",
    "    for feature in correlated_features:\n",
    "        # Check if the feature is already in a group\n",
    "        added = False\n",
    "        for group in grouped_features:\n",
    "            if feature in group:\n",
    "                added = True\n",
    "                break\n",
    "\n",
    "        # If the feature is not in any group, create a new group\n",
    "        if not added:\n",
    "            new_group = [feature]\n",
    "            # Add correlated features to the group\n",
    "            for correlated_feature in correlated_features:\n",
    "                if correlated_feature != feature and corr_matrix.loc[feature, correlated_feature] > threshold:\n",
    "                    new_group.append(correlated_feature)\n",
    "            grouped_features.append(new_group)\n",
    "\n",
    "    return grouped_features\n",
    "\n",
    "# Function to iterate over feature groups, modify dataframe, and print outcome\n",
    "def iterate_feature_groups(X_train, X_test, y_train, y_test, feature_groups):\n",
    "    for group in feature_groups:\n",
    "        print(f\"Performance after removing features {group}:\")\n",
    "\n",
    "        # Modify the dataframes by dropping the current group of features\n",
    "        X_train_mod = modify_df(pd.DataFrame(X_train), group)\n",
    "        X_test_mod = modify_df(pd.DataFrame(X_test), group)\n",
    "\n",
    "        # Rescaling\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_array_mod = scaler.transform(X_train_mod.values)\n",
    "        X_test_array_mod = scaler.transform(X_test_mod.values)\n",
    "\n",
    "        # Load the preprocessed data\n",
    "        X_train_mod = X_train_array_mod\n",
    "        X_test_mod = X_test_array_mod\n",
    "\n",
    "        # Train Naive Bayes model on modified data\n",
    "        nb_classifier = train_naive_bayes_model(X_train_mod, y_train)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        evaluate_model(nb_classifier, X_test_mod, y_test)\n",
    "\n",
    "# Load your preprocessed data and define feature groups\n",
    "feature_groups = group_features(df_clean, threshold=0.8)\n",
    "\n",
    "# Usage:\n",
    "iterate_feature_groups(X_train_array, X_test_array, y_train, y_test, feature_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        16\n",
       "1         1\n",
       "2         1\n",
       "3        64\n",
       "4        16\n",
       "         ..\n",
       "63342    16\n",
       "63343     4\n",
       "63346     1\n",
       "63347     4\n",
       "63348     4\n",
       "Name: geog_family_size_y, Length: 53616, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['geog_family_size_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
